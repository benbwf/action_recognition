{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# notes\n",
    "changed to output method to use fps* clip_duration instead of checking the number of frames extracted from that clip, because \n",
    "for certain videos, extracting 2s worth of 30fps videos will result in more than 60 frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.vid2img import vid2jpg, convert_folder\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import time\n",
    "\n",
    "from ops.dataset import TSNDataSet\n",
    "from ops import dataset_config\n",
    "from ops.models import TSN\n",
    "from env_vars import VIDEOS_DIR, PREPROCESSED_DATA_ROOT, RAW_DATA_ROOT\n",
    "\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "from ops.transforms import *\n",
    "from torch.nn import functional as F #for softmax \n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "         correct_k = correct[:k].contiguous().view(-1).float().sum(0)\n",
    "         res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_shift_option_from_log_name(log_name):\n",
    "    if 'shift' in log_name:\n",
    "        strings = log_name.split('_')\n",
    "        for i, s in enumerate(strings):\n",
    "            if 'shift' in s:\n",
    "                break\n",
    "        return True, int(strings[i].replace('shift', '')), strings[i + 1]\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "\n",
    "def eval_video(video_data, net, this_test_segments, modality):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        i, data, label = video_data\n",
    "        batch_size = label.numel()\n",
    "        num_crop = test_crops\n",
    "        if dense_sample:\n",
    "            num_crop *= 10  # 10 clips for testing when using dense sample\n",
    "\n",
    "        if twice_sample:\n",
    "            num_crop *= 2\n",
    "\n",
    "        if modality == 'RGB':\n",
    "            length = 3\n",
    "        elif modality == 'Flow':\n",
    "            length = 10\n",
    "        elif modality == 'RGBDiff':\n",
    "            length = 18\n",
    "        else:\n",
    "            raise ValueError(\"Unknown modality \"+ modality)\n",
    "\n",
    "        data_in = data.view(-1, length, data.size(2), data.size(3))\n",
    "        if is_shift:\n",
    "            data_in = data_in.view(batch_size * num_crop, this_test_segments, length, data_in.size(2), data_in.size(3))\n",
    "        rst = net(data_in)\n",
    "        rst = rst.reshape(batch_size, num_crop, -1).mean(1)\n",
    "\n",
    "        if softmax:\n",
    "            # take the softmax to normalize the output to probability\n",
    "            rst = F.softmax(rst, dim=1)\n",
    "\n",
    "        rst = rst.data.cpu().numpy().copy()\n",
    "\n",
    "        if net.module.is_shift:\n",
    "            rst = rst.reshape(batch_size, num_class)\n",
    "        else:\n",
    "            rst = rst.reshape((batch_size, -1, num_class)).mean(axis=1).reshape((batch_size, num_class))\n",
    "\n",
    "        return i, rst, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATASET VERSIONS, WEIGHTS TO LOAD, SOURCE_FOLDERS, DEST_FOLDERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dataset_version = '20210702_rev_all_actions_new_data_ek_ITE_batch1'\n",
    "dataset_version = '20210709_rev_all_actions_new_data_ek_ITE_batch2'\n",
    "\n",
    "# all actions\n",
    "this_weights = f'pretrained/fully_trained/01_all_actions/'\\\n",
    "               f'20210710_TSM_ite_RGB_resnet50_shift8_blockres_avg_segment8_e150_{dataset_version}_dense/'\\\n",
    "                'checkpoint/ckpt.best.pth.tar'\n",
    "\n",
    "#20210703_TSM_ite_RGB_resnet50_shift8_blockres_avg_segment8_e150_20210702_rev_all_actions_new_data_ek_ITE_batch1_dense\n",
    "#20210710_TSM_ite_RGB_resnet50_shift8_blockres_avg_segment8_e150_20210709_rev_all_actions_new_data_ek_ITE_batch2_dense\n",
    "\n",
    "# FOLDERS========================================================================================\n",
    "# video locations for dividing video into clips for getting the predictions\n",
    "\n",
    "source_dir = os.path.join(RAW_DATA_ROOT, 'ite_dataset', 'videos') #'demo_videos'\n",
    "test_csv_filepath = os.path.join(PREPROCESSED_DATA_ROOT, 'ite_dataset', dataset_version, 'test.csv')  # test.csv\n",
    "video_paths = []\n",
    "\n",
    "with open(test_csv_filepath, 'r') as test_csv_file:\n",
    "    for line in test_csv_file.readlines():\n",
    "        video_filename = line.strip()\n",
    "        result_paths = glob.glob(os.path.join(source_dir, '**', f'*{video_filename}'), recursive =True)\n",
    "        if len(result_paths) == 1:\n",
    "            video_paths.append(result_paths[0])\n",
    "        else:\n",
    "            print(f'Error! found 0 or more than 1 video file for test video {video_filename}')\n",
    "\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Fixed param, where the ite_dataset is at\n",
    "root_data_path = PREPROCESSED_DATA_ROOT\n",
    "\n",
    "# folder to store the prediction .csv and video files\n",
    "dt = time.strftime('%Y%m%d%H%M', time.localtime())\n",
    "output_path = os.path.join(cwd, f'{dt}_{dataset_version}_nseg8_ITE_VIDEO_TEST_results') #- used for just dense, full_res = False, test_crops=1\n",
    "#output_path = os.path.join(cwd, '202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results')\n",
    "\n",
    "Path(output_path).mkdir(exist_ok=True)\n",
    "\n",
    "# temp directory location for storing the split video from source_dir into 2s clips\n",
    "tmp_dir = os.path.join(PREPROCESSED_DATA_ROOT, 'tmp')\n",
    "Path(tmp_dir).mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210628\\\\ALL\\\\Student2.MP4',\n",
       " 'F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210629\\\\ghost\\\\edric_290621.MP4',\n",
       " 'F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210630\\\\ordeo_239\\\\irfanakid1_300621.MP4',\n",
       " 'F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210629\\\\ghost\\\\jason_290621.MP4',\n",
       " 'F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210629\\\\ghost\\\\mingzhi2_290621.MP4',\n",
       " 'F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210630\\\\ghost\\\\chanmunhong2_300621.MP4',\n",
       " 'F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210629\\\\ordeo_237\\\\decanay_290621.MP4',\n",
       " 'F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210630\\\\ghost\\\\daniel3_300621.MP4',\n",
       " 'F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210630\\\\ordeo_239\\\\lohshengyi1_300621.MP4',\n",
       " 'F:\\\\DS_dev\\\\data_raw\\\\ite_dataset\\\\videos\\\\210630\\\\ghost\\\\yihong2_300621.MP4']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\DS_data'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root_data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PARAMETERS\n",
    "- Inferred by this_weights\n",
    "- Set by User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================================================#\n",
    "# Fixed parameters based on the model trained\n",
    "num_segments = 8\n",
    "\n",
    "#============================================================================================================#\n",
    "# Currently tested parameters that can be changed\n",
    "# FOR A MODEL THAT IS TRAINED WITH DENSE_SAMPLE, MUST SET EITHER TO TRUE\n",
    "dense_sample = True  # True \n",
    "twice_sample = False\n",
    "\n",
    "CLIP_DURATION = 2 #duration of clip to send to model for prediction\n",
    "\n",
    "#============================================================================================================#\n",
    "# Parameters that were fixed throughout different models prediction (perhaps could be altered for performance)\n",
    "# Are these training parameters as well?????\n",
    "test_crops = 1  #1\n",
    "full_res = False #False\n",
    "this_test_segments = 8\n",
    "#========================# Data Loading, Etc Parameters (changes based on computer, etc)==========================#\n",
    "batch_size = 2\n",
    "num_workers = 0\n",
    "\n",
    "#============================================================================================================#\n",
    "# Fixed parameters (either from parsing from this_weights, or not really changed)\n",
    "is_shift, shift_div, shift_place = parse_shift_option_from_log_name(this_weights)\n",
    "softmax = True\n",
    "SOFTMAX_THRESH = 0.8\n",
    "\n",
    "#============================================================================================================#\n",
    "# Check for device\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ite: 33 classes\n",
      "\n",
      "    Initializing TSN with base model: resnet50.\n",
      "    TSN Configurations:\n",
      "        input_modality:     RGB\n",
      "        num_segments:       8\n",
      "        new_length:         1\n",
      "        consensus_module:   avg\n",
      "        dropout_ratio:      0.8\n",
      "        img_feature_dim:    256\n",
      "            \n",
      "=> base model: resnet50\n",
      "Adding temporal shift...\n",
      "=> n_segment per stage: [8, 8, 8, 8]\n",
      "=> Processing stage with 3 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Processing stage with 4 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Processing stage with 6 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Processing stage with 3 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torchvision\\transforms\\transforms.py:257: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "this_arch = this_weights.split('TSM_')[1].split('_')[2]\n",
    "if 'RGB' in this_weights:\n",
    "        modality = 'RGB'\n",
    "else:\n",
    "    modality = 'Flow'\n",
    "    \n",
    "num_class, train_list, val_list, root_path, prefix = dataset_config.return_dataset(root_data_path, 'ite',\n",
    "                                                                                            modality, version = dataset_version)\n",
    "\n",
    "net = TSN(num_class, this_test_segments if is_shift else 1, modality,\n",
    "              base_model=this_arch,\n",
    "              consensus_type='avg',\n",
    "              img_feature_dim=256,\n",
    "              pretrain='imagenet',\n",
    "              is_shift=is_shift, shift_div=shift_div, shift_place=shift_place,\n",
    "              non_local='_nl' in this_weights,\n",
    "              )\n",
    "\n",
    "if 'tpool' in this_weights:\n",
    "    from ops.temporal_shift import make_temporal_pool\n",
    "    make_temporal_pool(net.base_model, this_test_segments)  # since DataParallel\n",
    "\n",
    "checkpoint = torch.load(this_weights, map_location=torch.device(dev))\n",
    "checkpoint = checkpoint['state_dict']\n",
    "\n",
    "# base_dict = {('base_model.' + k).replace('base_model.fc', 'new_fc'): v for k, v in list(checkpoint.items())}\n",
    "base_dict = {'.'.join(k.split('.')[1:]): v for k, v in list(checkpoint.items())}\n",
    "replace_dict = {'base_model.classifier.weight': 'new_fc.weight',\n",
    "                'base_model.classifier.bias': 'new_fc.bias',\n",
    "                }\n",
    "for k, v in replace_dict.items():\n",
    "    if k in base_dict:\n",
    "        base_dict[v] = base_dict.pop(k)\n",
    "\n",
    "net.load_state_dict(base_dict)\n",
    "\n",
    "input_size = net.scale_size if full_res else net.input_size\n",
    "if test_crops == 1:\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupScale(net.scale_size),\n",
    "        GroupCenterCrop(input_size),\n",
    "    ])\n",
    "elif test_crops == 3:  # do not flip, so only 5 crops\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupFullResSample(input_size, net.scale_size, flip=False)\n",
    "    ])\n",
    "elif test_crops == 5:  # do not flip, so only 5 crops\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupOverSample(input_size, net.scale_size, flip=False)\n",
    "    ])\n",
    "elif test_crops == 10:\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupOverSample(input_size, net.scale_size)\n",
    "    ])\n",
    "else:\n",
    "    raise ValueError(\"Only 1, 5, 10 crops are supported while we got {}\".format(test_crops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\DS_data\\\\tmp'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data and create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prepare_video(video_path, preprocess = True, dataloaders = True, del_clips = True):\n",
    "    filename = os.path.basename(video_path)\n",
    "    \n",
    "    #get video properties\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   # float `width`\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) ) # float `height`\n",
    "    cap.release()\n",
    "    \n",
    "    #convert video to clips\n",
    "    dest_dir = os.path.join(tmp_dir, filename)\n",
    "    print(dest_dir)\n",
    "    print(f'fps: {fps}, frame count: {frame_count}, clip duration: {CLIP_DURATION}, filename: {filename}')\n",
    "    \n",
    "    if preprocess:\n",
    "        for i in range(0, int(frame_count/fps), int(CLIP_DURATION)):\n",
    "            Path(dest_dir).mkdir(exist_ok=True)\n",
    "            target_filepath = os.path.join(dest_dir, f'{i:04}_{filename}')\n",
    "            \n",
    "            # convert clips to images\n",
    "            ffmpeg_extract_subclip(video_path, i, i + int(CLIP_DURATION), targetname=target_filepath)\n",
    "\n",
    "        #convert clips to images\n",
    "        convert_folder(dest_dir, dest_dir)\n",
    "    \n",
    "    #generate video file list\n",
    "    video_folders_file = 'videofolder.txt'\n",
    "    video_folders_filepath = os.path.join(dest_dir, video_folders_file)\n",
    "    with open(video_folders_filepath, 'w+') as file:\n",
    "        for folder in glob.glob(os.path.join(dest_dir, '*')):\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            num_images = len(glob.glob(os.path.join(folder, '*')))\n",
    "            file.write(f'{folder},{num_images},{-1}\\n')\n",
    "            \n",
    "    if del_clips:\n",
    "        for p in glob.glob(os.path.join(dest_dir, '*.MP4')):\n",
    "            os.remove(p)\n",
    "            \n",
    "    #prepare data loaders\n",
    "    if dataloaders:\n",
    "        print(f'net.input_mean: {net.input_mean}, net.input_std: {net.input_std}')\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "                    TSNDataSet(root_path, video_folders_filepath, num_segments=num_segments,\n",
    "                               new_length=1 if modality == \"RGB\" else 5,\n",
    "                               modality=modality,\n",
    "                               image_tmpl=prefix,\n",
    "                               test_mode=True,\n",
    "                               random_shift = False, #use consistent spacing between segments (frames)\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   cropping,\n",
    "                                   Stack(roll=(this_arch in ['BNInception', 'InceptionV3'])),\n",
    "                                   ToTorchFormatTensor(div=(this_arch not in ['BNInception', 'InceptionV3'])),\n",
    "                                   GroupNormalize([0.485, 0.456, 0.406], [0.485, 0.456, 0.406]),\n",
    "                               ]), dense_sample=dense_sample, twice_sample=twice_sample),\n",
    "                    batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=num_workers, pin_memory=True,\n",
    "            )\n",
    "    else:\n",
    "        data_loader = None\n",
    "        \n",
    "    return {'data_loader': data_loader, 'videofolder':video_folders_filepath, 'fps':fps, 'frame_count':frame_count, 'width': width, 'height':height}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\DS_data\\tmp\\Student2.MP4\n",
      "fps: 30.0, frame count: 45915, clip duration: 2, filename: Student2.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:765\n",
      "C:\\DS_data\\tmp\\edric_290621.MP4\n",
      "fps: 29.97002997002997, frame count: 35488, clip duration: 2, filename: edric_290621.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:592\n",
      "C:\\DS_data\\tmp\\irfanakid1_300621.MP4\n",
      "fps: 30.0, frame count: 25290, clip duration: 2, filename: irfanakid1_300621.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:422\n",
      "C:\\DS_data\\tmp\\jason_290621.MP4\n",
      "fps: 29.97002997002997, frame count: 25006, clip duration: 2, filename: jason_290621.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:417\n",
      "C:\\DS_data\\tmp\\mingzhi2_290621.MP4\n",
      "fps: 29.97002997002997, frame count: 28496, clip duration: 2, filename: mingzhi2_290621.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:475\n",
      "C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\n",
      "fps: 29.97, frame count: 65092, clip duration: 2, filename: chanmunhong2_300621.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:1086\n",
      "C:\\DS_data\\tmp\\decanay_290621.MP4\n",
      "fps: 30.0, frame count: 39630, clip duration: 2, filename: decanay_290621.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:661\n",
      "C:\\DS_data\\tmp\\daniel3_300621.MP4\n",
      "fps: 29.97002997002997, frame count: 38781, clip duration: 2, filename: daniel3_300621.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:647\n",
      "C:\\DS_data\\tmp\\lohshengyi1_300621.MP4\n",
      "fps: 30.0, frame count: 51960, clip duration: 2, filename: lohshengyi1_300621.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:866\n",
      "C:\\DS_data\\tmp\\yihong2_300621.MP4\n",
      "fps: 29.97002997002997, frame count: 29805, clip duration: 2, filename: yihong2_300621.MP4\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:497\n"
     ]
    }
   ],
   "source": [
    "video_dict = dict()\n",
    "for video_path in video_paths:\n",
    "    video_dict[video_path] = prepare_video(video_path, preprocess = False, dataloaders = True, del_clips=False) \n",
    "    #video_dict[video_path] = prepare_video(video_path, preprocess = False, dataloaders = True, del_clips=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = torch.nn.DataParallel(net.to(dev))\n",
    "net.eval()\n",
    "for video_path in video_paths:\n",
    "    data_loader = video_dict[video_path]['data_loader']\n",
    "    data_gen = enumerate(data_loader)\n",
    "    this_rst_list = []\n",
    "    label_list = []\n",
    "    for i, (data, label) in data_gen:\n",
    "        rst = eval_video((i, data, label), net, this_test_segments, modality)\n",
    "\n",
    "        for l, r in zip(label, rst[1]): #unpack batch to individual samples\n",
    "            #save to lists\n",
    "            this_rst_list.append(r) \n",
    "            label_list.append(l) \n",
    "            \n",
    "    video_dict[video_path]['preds']= this_rst_list\n",
    "    video_dict[video_path]['labels']= label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save using pickle.....have to stop this to do object placement\n",
    "import pickle\n",
    "Path(output_path).mkdir(exist_ok=True)\n",
    "file_to_write = open( os.path.join(output_path, 'prediction_results'), \"wb\")\n",
    "pickle.dump(video_dict, file_to_write)\n",
    "\n",
    "file_to_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading video_dict and all its predictions so as to not go through predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### variables required?\n",
    "cwd = \n",
    "output_path =  os.path.join(cwd, f'{dt}_{dataset_version}_nseg{num_segments}_ITE_VIDEO_results')\n",
    "\n",
    "##### please load categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "file_to_read = open(os.path.join(output_path, 'prediction_results'), \"rb\")\n",
    "\n",
    "video_dict = pickle.load(file_to_read)\n",
    "\n",
    "file_to_read.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load section time info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load scoring table, then get section time info from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "sequence_dict, marks_dict, section_times_dict = {}, {}, {}\n",
    "section_dict = defaultdict(list)\n",
    "sect_dict = defaultdict(list)\n",
    "with open('scoring_table_danny.csv', 'r') as csv_file:\n",
    "    lines = csv_file.readlines()\n",
    "    for line in lines[1:]:\n",
    "#         print(line)\n",
    "        section, action, sequence, class_type, time_span, mark = line.split(',')\n",
    "        if class_type == 'action':\n",
    "            action_section = f'{action}_{section}'\n",
    "            sequence_dict[action_section] = int(sequence)\n",
    "            marks_dict[action_section] = int(mark)\n",
    "            sect_dict[action].append(section)\n",
    "            start, end = time_span.split('-')\n",
    "            section_times_dict[section] = {'start':float(start), 'end':float(end)}\n",
    "            if section not in section_dict[action]:\n",
    "                section_dict[action].append(section)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': {'start': 0.0, 'end': 4.0},\n",
       " 'B': {'start': 0.0, 'end': 10.0},\n",
       " 'C': {'start': 4.0, 'end': 20.0},\n",
       " 'E': {'start': 8.0, 'end': 35.0},\n",
       " 'F': {'start': 15.0, 'end': 40.0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "section_times_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "section_timespans = []\n",
    "for k, v in section_times_dict.items():\n",
    "    section_timespans.append(((v['start'], v['end']), k))\n",
    "for video_path in video_paths:\n",
    "    video_dict[video_path]['sect_timespans'] = section_timespans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read label map:\n",
    "with open(os.path.join(root_data_path, 'ite_dataset', dataset_version, 'actions_label_map.txt'), 'r') as file:\n",
    "    categories = file.readlines()\n",
    "    categories = [c.strip().replace(' ', '_').replace('\"', '').replace('(', '').replace(')', '').replace(\"'\", '') for c in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['connect_alligator_clip',\n",
       " 'connect_atx_cable',\n",
       " 'connect_display_cable',\n",
       " 'connect_hdd_data_cable',\n",
       " 'connect_hdd_power_cable',\n",
       " 'connect_odd_data_cable',\n",
       " 'connect_odd_power_cable',\n",
       " 'disconnect_atx_cable',\n",
       " 'disconnect_display_cable',\n",
       " 'disconnect_hdd_data_cable',\n",
       " 'disconnect_hdd_power_cable',\n",
       " 'disconnect_odd_data_cable',\n",
       " 'disconnect_odd_power_cable',\n",
       " 'enter_bios_setup_mode',\n",
       " 'insert_hdd',\n",
       " 'insert_odd',\n",
       " 'insert_ram',\n",
       " 'insert_vga',\n",
       " 'login_screen',\n",
       " 'no_action',\n",
       " 'place_anti_static_mat',\n",
       " 'put_back_pc_casing',\n",
       " 'remove_hdd',\n",
       " 'remove_odd',\n",
       " 'remove_pc_casing',\n",
       " 'remove_ram',\n",
       " 'remove_vga',\n",
       " 'switch_off_power',\n",
       " 'switch_off_power_source',\n",
       " 'turn_on_pc',\n",
       " 'unplug_power_cable',\n",
       " 'verify_boot_sequence',\n",
       " 'wear_wrist_wrap']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_actions_from_sections(inp_sections):\n",
    "    \"\"\"\n",
    "    Gets list of actions for list of sections\n",
    "    input: list of sections. E.g: ['A', 'B']\n",
    "    returns: list of actions\n",
    "    \"\"\"\n",
    "    ret = list()\n",
    "    for action, sections in sect_dict.items():\n",
    "        if type(sections)!=list:\n",
    "            sections=[sections]\n",
    "        #check if the current action's sections are in queried list of sections:\n",
    "        #(https://www.geeksforgeeks.org/python-check-two-lists-least-one-element-common/)\n",
    "        if (set(sections)&set(inp_sections)):\n",
    "            ret.append(action)\n",
    "    return ret\n",
    "\n",
    "def get_sections_from_time(minute, section_timespans, offset=0):\n",
    "    \"\"\"\n",
    "    Gets list of sections that pertain to certain time\n",
    "    minute: current time of prediction\n",
    "    offset: offset to add to minute\n",
    "    \"\"\"\n",
    "    minute += offset\n",
    "    sections = list()\n",
    "    for timespan, section in section_timespans:\n",
    "        if timespan[0] <= minute < timespan[1]:\n",
    "            sections.append(section)\n",
    "    return sections\n",
    "\n",
    "def get_actions_from_time(minute, offset=0):\n",
    "    \"\"\"\n",
    "    gets list of actions that should be allowed to be predicted by the model for current time (minute)\n",
    "    \"\"\"\n",
    "    sections = get_sections_from_time(minute, offset)\n",
    "    actions = get_actions_from_sections(sections)\n",
    "    return actions\n",
    "\n",
    "def output_mask(actions):\n",
    "    \"\"\"\n",
    "    Gets mask to ignore actions\n",
    "    actions: list of actions to allow\n",
    "    \"\"\"   \n",
    "    return [1 if action in actions or action == 'no_action' else 0 for action in categories ]\n",
    "\n",
    "def get_video_properties(video_path):\n",
    "    #get video properties\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   # float `width`\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) ) # float `height`\n",
    "    cap.release()\n",
    "    return fps, frame_count, width, height\n",
    "\n",
    "def get_preds_from_csv(csv_path):\n",
    "    preds = list()\n",
    "    with open(csv_path) as file:\n",
    "        for line in file.readlines()[1:]:\n",
    "            pred = line.strip().split(',')[1:]\n",
    "#             pred = [float(i) for i in str_pred.split(',')]\n",
    "            preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "def process_preds(preds, softmax_thresh=False, suppress_to = None):\n",
    "    pred_idxs = []\n",
    "    for pred in preds:\n",
    "        if softmax_thresh:\n",
    "            if max(pred)<softmax_thresh:\n",
    "                pred_idxs.append(suppress_to)\n",
    "            else:\n",
    "                pred_idxs.append(np.argmax(pred))\n",
    "    return pred_idxs\n",
    "\n",
    "def get_clips_paths(video_folders_filepath):\n",
    "    #get video clip paths from videofolder.txt\n",
    "    clip_paths = list()\n",
    "    with open(video_folders_filepath, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            clip_path = line.split(',')[0]+'.MP4'\n",
    "            clip_paths.append(clip_path)\n",
    "    return clip_paths\n",
    "\n",
    "def get_clips_lengths(video_folders_filepath):\n",
    "    #get video clip paths from videofolder.txt\n",
    "    clip_lengths = list()\n",
    "    with open(video_folders_filepath, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            clip_num_frames = int(line.split(',')[1])\n",
    "            clip_lengths.append(clip_num_frames)\n",
    "    return clip_lengths\n",
    "\n",
    "def save_preds_to_csv(video_path):\n",
    "    file_name = os.path.basename(video_path)\n",
    "    video_folders_filepath = video_dict[video_path]['videofolder']\n",
    "#     print(video_folders_filepath)\n",
    "    preds = video_dict[video_path]['preds']\n",
    "    labels = video_dict[video_path]['labels']\n",
    "        \n",
    "    #get video clip paths from videofolder.txt\n",
    "    clip_lengths = get_clips_lengths(video_folders_filepath)\n",
    "    print('number of preds: ', len(preds), 'number of clips: ',len(clip_lengths))\n",
    "    assert len(preds)==len(clip_lengths)\n",
    "    \n",
    "    # write results to csv file\n",
    "    csv_filepath = os.path.join(output_path, 'pred_'+file_name+'.csv')\n",
    "    csv_file = open(csv_filepath, 'w+')\n",
    "    cats_string = ','.join(categories)\n",
    "    csv_file.write(f'frame_pos,{cats_string}\\n')\n",
    "    i = 1\n",
    "    for raw_pred, label, clip_length in zip(preds, labels, clip_lengths):\n",
    "        #used to open clip to find number of frames\n",
    "        # now will just use value from videofolder.txt file to find that number\n",
    "        #get video properties\n",
    "        fps, frame_count, width, height = get_video_properties(video_path)\n",
    "        if clip_length > fps*CLIP_DURATION:\n",
    "            clip_length = fps*CLIP_DURATION #to avoid this problem: https://video.stackexchange.com/questions/23373/ffmpeg-not-creating-exact-duration-clip\n",
    "            \n",
    "        for j in range(int(clip_length)):\n",
    "            pred_str = ','.join([str(i) for i in raw_pred])\n",
    "            csv_file.write(f'{i},{pred_str}\\n')\n",
    "            i+= 1\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of preds:  765 number of clips:  765\n",
      "number of preds:  592 number of clips:  592\n",
      "number of preds:  422 number of clips:  422\n",
      "number of preds:  417 number of clips:  417\n",
      "number of preds:  475 number of clips:  475\n",
      "number of preds:  1086 number of clips:  1086\n",
      "number of preds:  661 number of clips:  661\n",
      "number of preds:  647 number of clips:  647\n",
      "number of preds:  866 number of clips:  866\n",
      "number of preds:  497 number of clips:  497\n"
     ]
    }
   ],
   "source": [
    "for video_path in video_paths:\n",
    "    save_preds_to_csv(video_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process predictions \n",
    "Split by section using time, then suppress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_THRES = 0.4 # previously 0.5 lead to worse scores than no confidence threshold\n",
    "USE_CONF_THRES = False\n",
    "\n",
    "USE_SECTION_SUPPRESION = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n",
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n",
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n",
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n",
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n",
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n",
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n",
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n",
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n",
      "[((0.0, 4.0), 'A'), ((0.0, 10.0), 'B'), ((4.0, 20.0), 'C'), ((8.0, 35.0), 'E'), ((15.0, 40.0), 'F')]\n"
     ]
    }
   ],
   "source": [
    "for video_path in video_paths:\n",
    "    #read predictions from csv:\n",
    "    file_name = os.path.basename(video_path)\n",
    "    csv_filepath = os.path.join(output_path, 'pred_'+ file_name +'.csv')\n",
    "    preds = get_preds_from_csv(csv_filepath)    \n",
    "\n",
    "    section_timespans = video_dict[video_path]['sect_timespans']\n",
    "    print(section_timespans)\n",
    "    \n",
    "    #get video properties\n",
    "    fps, frame_count, width, height = get_video_properties(video_path)\n",
    "    \n",
    "    #process predictions\n",
    "    processed_preds = list()\n",
    "    predicted_actions = list()\n",
    "    pred_act_sect = list()\n",
    "    per_frame_sections = list()\n",
    "    processed_preds_unmasked = list()\n",
    "    \n",
    "    for idx, pred in enumerate(preds):\n",
    "        pred = [float(p) for p in pred]\n",
    "        #current minute into the video\n",
    "        current_minute = idx/fps/60\n",
    "        \n",
    "        #get allowed sections and actions for current time\n",
    "        sections = get_sections_from_time(current_minute, section_timespans)\n",
    "        actions = get_actions_from_sections(sections)\n",
    "        \n",
    "        #get output mask\n",
    "        pred_mask = output_mask(actions)\n",
    "        \n",
    "        if USE_SECTION_SUPPRESION:\n",
    "            #mask out invalid predictions (out of section, etc)\n",
    "            masked_preds = np.array(pred)*np.array(pred_mask)\n",
    "        else:\n",
    "            # use whatever raw prediction that comes out\n",
    "            masked_preds = np.array(pred)\n",
    "        \n",
    "      \n",
    "        if USE_CONF_THRES:\n",
    "            # for actions (suppressed or not), check whether it meets a confidence criteria --- ADDED 22/6/2021\n",
    "            if np.max(masked_preds) > CONF_THRES:\n",
    "                predicted_action = categories[np.argmax(masked_preds)]      \n",
    "            else:\n",
    "                predicted_action = 'no_action'\n",
    "        else:\n",
    "            #get predicted action --- ORIGINAL, 22/6/2021\n",
    "            predicted_action = categories[np.argmax(masked_preds)]\n",
    "            \n",
    "        predicted_action_no_mask = categories[np.argmax(pred)]\n",
    "            \n",
    "            \n",
    "                \n",
    "        #find the section that the action belongs to\n",
    "        #some actions may belong to multiple sections\n",
    "        #so we find the overlap from get_sections_from_time and the sect_dict\n",
    "        sections_for_this_action = sect_dict[predicted_action]\n",
    "        #put into list if it isn't\n",
    "        if type(sections_for_this_action)!=list:\n",
    "            sections_for_this_action=[sections_for_this_action]\n",
    "        current_section = set(sections_for_this_action)&set(sections)\n",
    "        current_section = ''.join(list(current_section))\n",
    "        \n",
    "#         if idx%600==0:\n",
    "#             print(current_section)\n",
    "#             print(f'{predicted_action}_{current_section}')\n",
    "\n",
    "        per_frame_sections.append(current_section)\n",
    "        processed_preds.append(masked_preds)\n",
    "        predicted_actions.append(predicted_action)\n",
    "        pred_act_sect.append(f'{predicted_action}_{current_section}')\n",
    "        processed_preds_unmasked.append(predicted_action_no_mask)\n",
    "        \n",
    "    video_dict[video_path]['processed_preds'] = processed_preds\n",
    "    video_dict[video_path]['predicted_actions'] = predicted_actions\n",
    "    video_dict[video_path]['pred_act_sect'] = pred_act_sect\n",
    "    video_dict[video_path]['per_frame_sections'] = per_frame_sections\n",
    "    video_dict[video_path]['processed_preds_unmasked'] = processed_preds_unmasked\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write processed predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45900\n",
      "34928\n",
      "25290\n",
      "24603\n",
      "28025\n",
      "64072\n",
      "39630\n",
      "38173\n",
      "51960\n",
      "29323\n"
     ]
    }
   ],
   "source": [
    "for video_path in video_paths:\n",
    "    #read predictions from csv:\n",
    "    file_name = os.path.basename(video_path)\n",
    "    csv_filepath = os.path.join(output_path, 'processed_pred_'+file_name+'.csv')\n",
    "    act_sect_preds = video_dict[video_path]['pred_act_sect']\n",
    "    print(len(act_sect_preds))\n",
    "    with open(csv_filepath, 'w+') as file:\n",
    "        file.write(f'frame_pos,pred\\n')\n",
    "        for idx, pred in enumerate(act_sect_preds):\n",
    "            file.write(f'{idx+1},{pred}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write processed unmasked predictions to CSV\n",
    "(without timespan filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45900\n",
      "34928\n",
      "25290\n",
      "24603\n",
      "28025\n",
      "64072\n",
      "39630\n",
      "38173\n",
      "51960\n",
      "29323\n"
     ]
    }
   ],
   "source": [
    "for video_path in video_paths:\n",
    "    #read predictions from csv:\n",
    "    file_name = os.path.basename(video_path)\n",
    "    csv_filepath = os.path.join(output_path, 'processed_pred_unmasked_'+file_name+'.csv')\n",
    "    preds = video_dict[video_path]['processed_preds_unmasked']\n",
    "    print(len(preds))\n",
    "    with open(csv_filepath, 'w+') as file:\n",
    "        file.write(f'frame_pos,pred\\n')\n",
    "        for idx, pred in enumerate(preds):\n",
    "            file.write(f'{idx+1},{pred}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate section timings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for video_path in video_paths:\n",
    "    #read predictions from csv:\n",
    "    file_name = os.path.basename(video_path)\n",
    "    csv_filepath = os.path.join(output_path, 'processed_pred_'+file_name+'.csv')\n",
    "    act_sect_preds = video_dict[video_path]['pred_act_sect']\n",
    "#     print(act_sect_preds)\n",
    "    sects = [pred[-1] if 'no_action' not in pred else 0 for pred in act_sect_preds]\n",
    "    firsts_lasts_dict = {'A':{'first':None, 'last':None},\n",
    "                        'B':{'first':None, 'last':None},\n",
    "                       'C':{'first':None, 'last':None},\n",
    "                       'E':{'first':None, 'last':None},\n",
    "                       'F':{'first':None, 'last':None}}\n",
    "    \n",
    "    #find the first and last occurence of each section action\n",
    "    for i, section in enumerate(sects):\n",
    "        if section in firsts_lasts_dict.keys():\n",
    "            if firsts_lasts_dict[section]['first'] is None:\n",
    "                firsts_lasts_dict[section]['first'] = i\n",
    "            firsts_lasts_dict[section]['last'] = i\n",
    "    \n",
    "    #remove sections where no action was detected:\n",
    "    temp = dict()\n",
    "    for k, v in firsts_lasts_dict.items():\n",
    "        if v['first'] is not None:\n",
    "            temp[k] = v\n",
    "    firsts_lasts_dict = temp\n",
    "    \n",
    "    #calculate midpoint of overlaps\n",
    "    for i, (k, v) in enumerate(firsts_lasts_dict.items()):\n",
    "        if i < len(list(firsts_lasts_dict.keys()))-1:\n",
    "            current_last = v['last'] \n",
    "#                 current_last = current_last if current_last is not None else 0\n",
    "            next_sect = list(firsts_lasts_dict.keys())[i+1]\n",
    "            next_first = firsts_lasts_dict[next_sect]['first']\n",
    "#                 next_first = next_first if next_first is not None else 0\n",
    "    #         print(current_last, next_first, (current_last+next_first)//2, (current_last+next_first)//2+1)\n",
    "    #         print(k, next_sect)\n",
    "\n",
    "#             print(firsts_lasts_dict)\n",
    "            firsts_lasts_dict[k]['last'] = (current_last+next_first)//2\n",
    "            firsts_lasts_dict[next_sect]['first'] = (current_last+next_first)//2+1\n",
    "        fps = video_dict[video_path]['fps']\n",
    "        m = v['last']/fps/60\n",
    "        v['time'] = m\n",
    "#     print(video_path)\n",
    "#     print(firsts_lasts_dict)\n",
    "    #write to csv\n",
    "    \n",
    "    csv_filepath = os.path.join(output_path, 'section_timings_'+ file_name +'.csv')\n",
    "    with open(csv_filepath, 'w') as file:\n",
    "        file.write(f'section,minute_end\\n')\n",
    "        for s in ['A','B','C','E','F']:\n",
    "            if s in firsts_lasts_dict.keys():\n",
    "                time = firsts_lasts_dict[s]['time']\n",
    "                file.write(f'{s},{time}\\n')\n",
    "            else:\n",
    "                file.write(f'{s},{-1}\\n')\n",
    "        \n",
    "    video_dict[video_path]['section_times'] = firsts_lasts_dict\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write predictions to video\n",
    "### warning: will take long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing edric_290621.MP4\n",
      "34928 592\n",
      "34928 35488\n",
      "writing to: F:\\DS_dev\\temporal-shift-module\\202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results\\nodelay_pred_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1164_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1166_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1168_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1170_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1172_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1174_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1176_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1178_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1180_edric_290621.MP4\n",
      "pred limit reached. i=34928, clip_path = C:\\DS_data\\tmp\\edric_290621.MP4\\1182_edric_290621.MP4\n",
      "processing irfanakid1_300621.MP4\n",
      "25290 422\n",
      "25290 25290\n",
      "writing to: F:\\DS_dev\\temporal-shift-module\\202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results\\nodelay_pred_irfanakid1_300621.MP4\n",
      "processing jason_290621.MP4\n",
      "24603 417\n",
      "24603 25006\n",
      "writing to: F:\\DS_dev\\temporal-shift-module\\202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results\\nodelay_pred_jason_290621.MP4\n",
      "pred limit reached. i=24603, clip_path = C:\\DS_data\\tmp\\jason_290621.MP4\\0820_jason_290621.MP4\n",
      "pred limit reached. i=24603, clip_path = C:\\DS_data\\tmp\\jason_290621.MP4\\0822_jason_290621.MP4\n",
      "pred limit reached. i=24603, clip_path = C:\\DS_data\\tmp\\jason_290621.MP4\\0824_jason_290621.MP4\n",
      "pred limit reached. i=24603, clip_path = C:\\DS_data\\tmp\\jason_290621.MP4\\0826_jason_290621.MP4\n",
      "pred limit reached. i=24603, clip_path = C:\\DS_data\\tmp\\jason_290621.MP4\\0828_jason_290621.MP4\n",
      "pred limit reached. i=24603, clip_path = C:\\DS_data\\tmp\\jason_290621.MP4\\0830_jason_290621.MP4\n",
      "pred limit reached. i=24603, clip_path = C:\\DS_data\\tmp\\jason_290621.MP4\\0832_jason_290621.MP4\n",
      "processing mingzhi2_290621.MP4\n",
      "28025 475\n",
      "28025 28496\n",
      "writing to: F:\\DS_dev\\temporal-shift-module\\202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results\\nodelay_pred_mingzhi2_290621.MP4\n",
      "pred limit reached. i=28025, clip_path = C:\\DS_data\\tmp\\mingzhi2_290621.MP4\\0934_mingzhi2_290621.MP4\n",
      "pred limit reached. i=28025, clip_path = C:\\DS_data\\tmp\\mingzhi2_290621.MP4\\0936_mingzhi2_290621.MP4\n",
      "pred limit reached. i=28025, clip_path = C:\\DS_data\\tmp\\mingzhi2_290621.MP4\\0938_mingzhi2_290621.MP4\n",
      "pred limit reached. i=28025, clip_path = C:\\DS_data\\tmp\\mingzhi2_290621.MP4\\0940_mingzhi2_290621.MP4\n",
      "pred limit reached. i=28025, clip_path = C:\\DS_data\\tmp\\mingzhi2_290621.MP4\\0942_mingzhi2_290621.MP4\n",
      "pred limit reached. i=28025, clip_path = C:\\DS_data\\tmp\\mingzhi2_290621.MP4\\0944_mingzhi2_290621.MP4\n",
      "pred limit reached. i=28025, clip_path = C:\\DS_data\\tmp\\mingzhi2_290621.MP4\\0946_mingzhi2_290621.MP4\n",
      "pred limit reached. i=28025, clip_path = C:\\DS_data\\tmp\\mingzhi2_290621.MP4\\0948_mingzhi2_290621.MP4\n",
      "processing chanmunhong2_300621.MP4\n",
      "64072 1086\n",
      "64072 65092\n",
      "writing to: F:\\DS_dev\\temporal-shift-module\\202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results\\nodelay_pred_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2134_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2136_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2138_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2140_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2142_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2144_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2146_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2148_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2150_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2152_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2154_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2156_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2158_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2160_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2162_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2164_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2166_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2168_chanmunhong2_300621.MP4\n",
      "pred limit reached. i=64072, clip_path = C:\\DS_data\\tmp\\chanmunhong2_300621.MP4\\2170_chanmunhong2_300621.MP4\n",
      "processing decanay_290621.MP4\n",
      "39630 661\n",
      "39630 39630\n",
      "writing to: F:\\DS_dev\\temporal-shift-module\\202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results\\nodelay_pred_decanay_290621.MP4\n",
      "processing daniel3_300621.MP4\n",
      "38173 647\n",
      "38173 38781\n",
      "writing to: F:\\DS_dev\\temporal-shift-module\\202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results\\nodelay_pred_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1272_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1274_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1276_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1278_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1280_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1282_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1284_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1286_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1288_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1290_daniel3_300621.MP4\n",
      "pred limit reached. i=38173, clip_path = C:\\DS_data\\tmp\\daniel3_300621.MP4\\1292_daniel3_300621.MP4\n",
      "processing lohshengyi1_300621.MP4\n",
      "51960 866\n",
      "51960 51960\n",
      "writing to: F:\\DS_dev\\temporal-shift-module\\202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results\\nodelay_pred_lohshengyi1_300621.MP4\n",
      "processing yihong2_300621.MP4\n",
      "29323 497\n",
      "29323 29805\n",
      "writing to: F:\\DS_dev\\temporal-shift-module\\202107042048_20210702_rev_all_actions_new_data_ek_ITE_batch1_nseg8_ITE_VIDEO_TEST_results\\nodelay_pred_yihong2_300621.MP4\n",
      "pred limit reached. i=29323, clip_path = C:\\DS_data\\tmp\\yihong2_300621.MP4\\0978_yihong2_300621.MP4\n",
      "pred limit reached. i=29323, clip_path = C:\\DS_data\\tmp\\yihong2_300621.MP4\\0980_yihong2_300621.MP4\n",
      "pred limit reached. i=29323, clip_path = C:\\DS_data\\tmp\\yihong2_300621.MP4\\0982_yihong2_300621.MP4\n",
      "pred limit reached. i=29323, clip_path = C:\\DS_data\\tmp\\yihong2_300621.MP4\\0984_yihong2_300621.MP4\n",
      "pred limit reached. i=29323, clip_path = C:\\DS_data\\tmp\\yihong2_300621.MP4\\0986_yihong2_300621.MP4\n",
      "pred limit reached. i=29323, clip_path = C:\\DS_data\\tmp\\yihong2_300621.MP4\\0988_yihong2_300621.MP4\n",
      "pred limit reached. i=29323, clip_path = C:\\DS_data\\tmp\\yihong2_300621.MP4\\0990_yihong2_300621.MP4\n",
      "pred limit reached. i=29323, clip_path = C:\\DS_data\\tmp\\yihong2_300621.MP4\\0992_yihong2_300621.MP4\n"
     ]
    }
   ],
   "source": [
    "#TODO: read preds from processed_preds csv file \n",
    "# def write_preds_to_video(video_path):\n",
    "for video_path in video_paths[1:]: #video_paths[14:19] - TO SELECT A FEW\n",
    "    \n",
    "    file_name = os.path.basename(video_path)\n",
    "    print(f'processing {file_name}')\n",
    "    video_folders_filepath = video_dict[video_path]['videofolder']\n",
    "    \n",
    "    #get labels\n",
    "#     df = get_gt_onehot_labels(video_path)\n",
    "#     labels = df[df==1].stack().reset_index().drop(['level_0', 0],1).level_1\n",
    "    \n",
    "    #get preds\n",
    "    act_sect_preds = video_dict[video_path]['pred_act_sect']\n",
    "    preds = act_sect_preds#video_dict[video_path]['processed_preds']\n",
    "    \n",
    "    #get section labels\n",
    "    per_frame_sections = video_dict[video_path]['per_frame_sections']\n",
    "    \n",
    "    #get video clip paths from videofolder.txt\n",
    "    clip_paths = get_clips_paths(video_folders_filepath)\n",
    "    print(len(preds), len(clip_paths))\n",
    "#     assert len(preds)==len(clip_paths)\n",
    "    \n",
    "    fps, frame_count, width, height = get_video_properties(video_path)\n",
    "    print(len(preds),frame_count)\n",
    "    label_height = 60\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    \n",
    "    output_video_filepath = os.path.join(output_path, 'nodelay_pred_'+file_name)\n",
    "    out = cv2.VideoWriter(output_video_filepath ,fourcc, fps, (width, height+label_height))\n",
    "    print(f'writing to: {output_video_filepath}')\n",
    "\n",
    "    i=0\n",
    "    for clip_path in clip_paths:        \n",
    "        \n",
    "        #open clip video\n",
    "        current_clip_frame_count = 0\n",
    "        cap = cv2.VideoCapture(clip_path) \n",
    "        while True:\n",
    "            _, img = cap.read()  \n",
    "\n",
    "            if not _:\n",
    "#                 print(\"No Image\")\n",
    "                cap.release()\n",
    "                break\n",
    "            elif current_clip_frame_count >= CLIP_DURATION*fps:\n",
    "                cap.release()\n",
    "                break\n",
    "            elif i >= len(preds):\n",
    "                print(f'pred limit reached. i={i}, clip_path = {clip_path}')\n",
    "                cap.release()\n",
    "                break\n",
    "            pred = preds[i].split('_')[:-1]\n",
    "            pred = ' '.join(pred)\n",
    "            current_section = per_frame_sections[i]\n",
    "            \n",
    "#             print(current_section)\n",
    "#             if i%100==0:\n",
    "#                 print(pred)\n",
    "\n",
    "            label_area = np.zeros([label_height, width, 3]).astype('uint8') + 255\n",
    "    \n",
    "#             print(pred)\n",
    "\n",
    "            cv2.putText(label_area, \n",
    "                        f'Prediction: {pred}.  ',# ({max(pred):.6f}) Label: {labels[i]}',\n",
    "                        (5, int(label_height-20)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.7, \n",
    "                        (0, 0, 0), \n",
    "                        2)\n",
    "            img = np.concatenate((img, label_area), axis=0)\n",
    "            \n",
    "            out.write(img)\n",
    "            i+= 1\n",
    "            current_clip_frame_count += 1\n",
    "    out.release()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student2.MP4\n",
      "45900 45915\n",
      "edric_290621.MP4\n",
      "34928 35488\n",
      "irfanakid1_300621.MP4\n",
      "25290 25290\n",
      "jason_290621.MP4\n",
      "24603 25006\n",
      "mingzhi2_290621.MP4\n",
      "28025 28496\n",
      "chanmunhong2_300621.MP4\n",
      "64072 65092\n",
      "decanay_290621.MP4\n",
      "39630 39630\n",
      "daniel3_300621.MP4\n",
      "38173 38781\n",
      "lohshengyi1_300621.MP4\n",
      "51960 51960\n",
      "yihong2_300621.MP4\n",
      "29323 29805\n"
     ]
    }
   ],
   "source": [
    "#read actions_label_map.txt\n",
    "#read label map:\n",
    "#     with open(os.path.join(PREPROCESSED_DATA_ROOT, 'ite_dataset', 'actions_label_map.txt'), 'r') as file:\n",
    "#         categories = file.readlines()\n",
    "#         categories = [c.strip().replace(' ', '_').replace('\"', '').replace('(', '').replace(')', '').replace(\"'\", '') for c in categories]\n",
    "#or since categories has been read earlier in the notebook, don't need to read it here.\n",
    "\n",
    "iou_means = list()\n",
    "iou_per_class_all_videos_df = pd.DataFrame(columns=categories)\n",
    "\n",
    "for video_path in video_paths:\n",
    "    file_name = os.path.basename(video_path)\n",
    "    print(file_name)\n",
    "    \n",
    "    # read GT and convert to per-frame labels\n",
    "    gt_csv = video_path +'.csv'\n",
    "    gt_labels = pd.read_csv(gt_csv)\n",
    "    #remove no_action rows\n",
    "    gt_labels = gt_labels[gt_labels['action']!='no_action']\n",
    "    gt_labels.sort_values('z_start', inplace=True)\n",
    "    #get frame count and fps of video\n",
    "    frame_count = video_dict[video_path]['frame_count']\n",
    "    fps = video_dict[video_path]['fps']\n",
    "#     base_data = np.zeros((frame_count, len(categories)))\n",
    "    gt_labels_onehot = pd.DataFrame(0, index=np.arange(frame_count), columns=categories)\n",
    "    gt_labels_onehot.no_action = 1\n",
    "    for index, row in gt_labels.iterrows():\n",
    "        action, z_start, z_end = row\n",
    "        \n",
    "        #ignore action if it isn't in actions_label_map.txt\n",
    "        if action not in categories:\n",
    "            continue\n",
    "#         print(action, z_start, z_end)\n",
    "        frame_pos_start = int(z_start*fps)\n",
    "        frame_pos_end = int(z_end*fps)\n",
    "#         print(frame_pos_start, frame_pos_end)\n",
    "        gt_labels_onehot[action][frame_pos_start:frame_pos_end]=1\n",
    "        gt_labels_onehot.no_action[frame_pos_start:frame_pos_end]=0\n",
    "        \n",
    "    #read predictions file\n",
    "    output_csv_filepath = os.path.join(output_path, 'processed_pred_unmasked_'+ file_name + '.csv')\n",
    "    preds_per_frame = pd.read_csv(output_csv_filepath)\n",
    "    print(len(preds_per_frame), len(gt_labels_onehot))\n",
    "    \n",
    "#     assert len(preds_per_frame) == len(gt_labels_onehot)\n",
    "    #convert to one-hot labels\n",
    "    \n",
    "    preds_onehot = pd.DataFrame(0, index=np.arange(frame_count), columns=categories)\n",
    "    for index, row in preds_per_frame.iterrows():\n",
    "        frame_pos, action = row\n",
    "#         print(frame_pos, action)\n",
    "        preds_onehot[action][frame_pos-1] = 1\n",
    "    \n",
    "    intersection_per_class = (gt_labels_onehot*preds_onehot).sum()\n",
    "    union_per_class = (gt_labels_onehot|preds_onehot).sum()\n",
    "#     union_per_class = gt_labels_onehot.sum()+preds_onehot.sum()\n",
    "    iou_per_class = intersection_per_class/union_per_class\n",
    "    mean_iou = iou_per_class.mean()\n",
    "#     print(f'intersection: {intersection}, union: {union}')\n",
    "    iou_means.append(mean_iou)\n",
    "    iou_per_class_all_videos_df=iou_per_class_all_videos_df.append(iou_per_class, ignore_index=True)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_per_class_all_videos_df.to_csv('iou_per_class_all_videos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou mean per video:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.242401\n",
       "1    0.058902\n",
       "2    0.272405\n",
       "3    0.085856\n",
       "4    0.097485\n",
       "5    0.029131\n",
       "6    0.253188\n",
       "7    0.086856\n",
       "8    0.276567\n",
       "9    0.041162\n",
       "dtype: float64"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('iou mean per video:')\n",
    "iou_per_class_all_videos_df.mean(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou mean per class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "connect_alligator_clip        0.061950\n",
       "connect_atx_cable             0.102127\n",
       "connect_display_cable         0.070853\n",
       "connect_hdd_data_cable        0.097434\n",
       "connect_hdd_power_cable       0.138730\n",
       "connect_odd_data_cable        0.087476\n",
       "connect_odd_power_cable       0.140265\n",
       "disconnect_atx_cable          0.051385\n",
       "disconnect_display_cable      0.070887\n",
       "disconnect_hdd_data_cable     0.095071\n",
       "disconnect_hdd_power_cable    0.079208\n",
       "disconnect_odd_data_cable     0.090311\n",
       "disconnect_odd_power_cable    0.077021\n",
       "enter_bios_setup_mode         0.065044\n",
       "insert_hdd                    0.222426\n",
       "insert_odd                    0.104843\n",
       "insert_ram                    0.158235\n",
       "insert_vga                    0.156324\n",
       "login_screen                  0.191378\n",
       "no_action                     0.692466\n",
       "place_anti_static_mat         0.198856\n",
       "put_back_pc_casing            0.110267\n",
       "remove_hdd                    0.217084\n",
       "remove_odd                    0.116442\n",
       "remove_pc_casing              0.146839\n",
       "remove_ram                    0.140459\n",
       "remove_vga                    0.176327\n",
       "switch_off_power              0.237104\n",
       "switch_off_power_source       0.179162\n",
       "turn_on_pc                    0.150544\n",
       "unplug_power_cable            0.118213\n",
       "verify_boot_sequence          0.057056\n",
       "wear_wrist_wrap               0.179268\n",
       "dtype: float64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('iou mean per class:')\n",
    "iou_per_class_all_videos_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1443953083313043"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('model score:')\n",
    "iou_per_class_all_videos_df.mean(axis=1, skipna=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model output analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxes = []\n",
    "for video_path in video_paths:\n",
    "#     print(video_dict[video_path])\n",
    "    file_name = os.path.basename(video_path)\n",
    "    preds = video_dict[video_path]['preds']\n",
    "    for pred in preds:\n",
    "        maxes.append(max(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(maxes, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.boxplot(maxes)\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
