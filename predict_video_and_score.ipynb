{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "Notebook to predict actions in video, and to score the accuracy of the model using a IOU-like method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ops.dataset import TSNDataSet\n",
    "from ops import dataset_config\n",
    "from ops.models import TSN\n",
    "from ops.transforms import *\n",
    "from tools.vid2img import vid2jpg, convert_folder\n",
    "from env_vars import VIDEOS_DIR, PREPROCESSED_DATA_ROOT, RAW_DATA_ROOT\n",
    "\n",
    "import os\n",
    "import glob\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from moviepy.video.io.ffmpeg_tools import ffmpeg_extract_subclip\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import Normalize\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "from torch.nn import functional as F #for softmax "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "         correct_k = correct[:k].contiguous().view(-1).float().sum(0)\n",
    "         res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_shift_option_from_log_name(log_name):\n",
    "    if 'shift' in log_name:\n",
    "        strings = log_name.split('_')\n",
    "        for i, s in enumerate(strings):\n",
    "            if 'shift' in s:\n",
    "                break\n",
    "        return True, int(strings[i].replace('shift', '')), strings[i + 1]\n",
    "    else:\n",
    "        return False, None, None\n",
    "    \n",
    "\n",
    "def eval_video(video_data, net, this_test_segments, modality):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        i, data, label = video_data\n",
    "        batch_size = label.numel()\n",
    "        num_crop = test_crops\n",
    "        if dense_sample:\n",
    "            num_crop *= 10  # 10 clips for testing when using dense sample\n",
    "\n",
    "        if twice_sample:\n",
    "            num_crop *= 2\n",
    "\n",
    "        if modality == 'RGB':\n",
    "            length = 3\n",
    "        elif modality == 'Flow':\n",
    "            length = 10\n",
    "        elif modality == 'RGBDiff':\n",
    "            length = 18\n",
    "        else:\n",
    "            raise ValueError(\"Unknown modality \"+ modality)\n",
    "\n",
    "        data_in = data.view(-1, length, data.size(2), data.size(3))\n",
    "        if is_shift:\n",
    "            data_in = data_in.view(batch_size * num_crop, this_test_segments, length, data_in.size(2), data_in.size(3))\n",
    "        rst = net(data_in)\n",
    "        rst = rst.reshape(batch_size, num_crop, -1).mean(1)\n",
    "\n",
    "        if softmax:\n",
    "            # take the softmax to normalize the output to probability\n",
    "            rst = F.softmax(rst, dim=1)\n",
    "\n",
    "        rst = rst.data.cpu().numpy().copy()\n",
    "\n",
    "        if net.module.is_shift:\n",
    "            rst = rst.reshape(batch_size, num_class)\n",
    "        else:\n",
    "            rst = rst.reshape((batch_size, -1, num_class)).mean(axis=1).reshape((batch_size, num_class))\n",
    "\n",
    "        return i, rst, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change these parameters as needed\n",
    "Set your dataset_version, weights path (this_weights), source directory, and output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_version = 'pc_101'\n",
    "\n",
    "# set to the path to your model file\n",
    "this_weights = f'checkpoint/'\\\n",
    "               f'20210710_TSM_ite_RGB_resnet50_shift8_blockres_avg_segment8_e150_{dataset_version}_dense/'\\\n",
    "                'checkpoint/ckpt.best.pth.tar'\n",
    "\n",
    "#duration of clip to send to model for prediction\n",
    "#this should represent the rough average length of your actions in the dataset.\n",
    "# example: if your actions are an average of 3 seconds long, then set CLIP_DURATION = 3\n",
    "CLIP_DURATION = 2 #seconds\n",
    "\n",
    "\n",
    "# FOLDERS========================================================================================\n",
    "# video locations for dividing video into clips for getting the predictions\n",
    "\n",
    "source_dir = os.path.join(RAW_DATA_ROOT, 'ite_dataset', 'videos', 'UAT_Stitched_Test_Cases')\n",
    "source_dir = \"C:\\\\Users\\\\User1\\\\Desktop\\\\projects\\\\ITE_APAMS\\\\ite_dataset\\\\videos\\\\210304\\\\A\"\n",
    "video_paths = glob.glob(os.path.join(source_dir, '*.MP4'))\n",
    "\n",
    "\n",
    "# Fixed param, where the ite_dataset is at\n",
    "root_data_path = PREPROCESSED_DATA_ROOT\n",
    "\n",
    "# folder to store the prediction .csv and video files\n",
    "dt = time.strftime('%Y%m%d%H%M', time.localtime())\n",
    "cwd = os.getcwd()\n",
    "output_path = os.path.join(cwd, f'{dt}_{dataset_version}_output') #- used for just dense, full_res = False, test_crops=1\n",
    "\n",
    "Path(output_path).mkdir(exist_ok=True)\n",
    "\n",
    "# temp directory location for storing the split video from source_dir into 2s clips\n",
    "tmp_dir = os.path.join(PREPROCESSED_DATA_ROOT, 'tmp')\n",
    "Path(tmp_dir).mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Users\\\\User1\\\\Desktop\\\\projects\\\\ITE_APAMS\\\\ite_dataset\\\\videos\\\\210304\\\\A\\\\A_1.MP4',\n",
       " 'C:\\\\Users\\\\User1\\\\Desktop\\\\projects\\\\ITE_APAMS\\\\ite_dataset\\\\videos\\\\210304\\\\A\\\\A_2.MP4',\n",
       " 'C:\\\\Users\\\\User1\\\\Desktop\\\\projects\\\\ITE_APAMS\\\\ite_dataset\\\\videos\\\\210304\\\\A\\\\A_3.MP4']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODEL PARAMETERS\n",
    "- Inferred by this_weights\n",
    "- Set by User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#============================================================================================================#\n",
    "# Fixed parameters based on the model trained\n",
    "num_segments = 8\n",
    "\n",
    "#============================================================================================================#\n",
    "# Currently tested parameters that can be changed\n",
    "# FOR A MODEL THAT IS TRAINED WITH DENSE_SAMPLE, MUST SET EITHER TO TRUE\n",
    "dense_sample = True  # True \n",
    "twice_sample = False\n",
    "\n",
    "#============================================================================================================#\n",
    "# Parameters that were fixed throughout different models prediction (perhaps could be altered for performance)\n",
    "# Are these training parameters as well?????\n",
    "test_crops = 1  #1\n",
    "full_res = False #False\n",
    "this_test_segments = 8\n",
    "#========================# Data Loading, Etc Parameters (changes based on computer, etc)==========================#\n",
    "batch_size = 2\n",
    "num_workers = 0\n",
    "\n",
    "#============================================================================================================#\n",
    "# Fixed parameters (either from parsing from this_weights, or not really changed)\n",
    "is_shift, shift_div, shift_place = parse_shift_option_from_log_name(this_weights)\n",
    "softmax = True\n",
    "SOFTMAX_THRESH = 0.8\n",
    "\n",
    "#============================================================================================================#\n",
    "# Check for device\n",
    "if torch.cuda.is_available():\n",
    "    dev = \"cuda:0\"\n",
    "else:\n",
    "    dev = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ite: 33 classes\n",
      "\n",
      "    Initializing TSN with base model: resnet50.\n",
      "    TSN Configurations:\n",
      "        input_modality:     RGB\n",
      "        num_segments:       8\n",
      "        new_length:         1\n",
      "        consensus_module:   avg\n",
      "        dropout_ratio:      0.8\n",
      "        img_feature_dim:    256\n",
      "            \n",
      "=> base model: resnet50\n",
      "Adding temporal shift...\n",
      "=> n_segment per stage: [8, 8, 8, 8]\n",
      "=> Processing stage with 3 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Processing stage with 4 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Processing stage with 6 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Processing stage with 3 blocks residual\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n",
      "=> Using fold div: 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\anaconda3\\envs\\ptcpu\\lib\\site-packages\\torchvision\\transforms\\transforms.py:280: UserWarning: Argument interpolation should be of type InterpolationMode instead of int. Please, use InterpolationMode enum.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "this_arch = this_weights.split('TSM_')[1].split('_')[2]\n",
    "if 'RGB' in this_weights:\n",
    "        modality = 'RGB'\n",
    "else:\n",
    "    modality = 'Flow'\n",
    "    \n",
    "num_class, train_list, val_list, root_path, prefix = dataset_config.return_dataset(root_data_path, 'ite',\n",
    "                                                                                            modality, version = dataset_version)\n",
    "\n",
    "net = TSN(num_class, this_test_segments if is_shift else 1, modality,\n",
    "              base_model=this_arch,\n",
    "              consensus_type='avg',\n",
    "              img_feature_dim=256,\n",
    "              pretrain='imagenet',\n",
    "              is_shift=is_shift, shift_div=shift_div, shift_place=shift_place,\n",
    "              non_local='_nl' in this_weights,\n",
    "              )\n",
    "\n",
    "if 'tpool' in this_weights:\n",
    "    from ops.temporal_shift import make_temporal_pool\n",
    "    make_temporal_pool(net.base_model, this_test_segments)  # since DataParallel\n",
    "\n",
    "checkpoint = torch.load(this_weights, map_location=torch.device(dev))\n",
    "checkpoint = checkpoint['state_dict']\n",
    "\n",
    "# base_dict = {('base_model.' + k).replace('base_model.fc', 'new_fc'): v for k, v in list(checkpoint.items())}\n",
    "base_dict = {'.'.join(k.split('.')[1:]): v for k, v in list(checkpoint.items())}\n",
    "replace_dict = {'base_model.classifier.weight': 'new_fc.weight',\n",
    "                'base_model.classifier.bias': 'new_fc.bias',\n",
    "                }\n",
    "for k, v in replace_dict.items():\n",
    "    if k in base_dict:\n",
    "        base_dict[v] = base_dict.pop(k)\n",
    "\n",
    "net.load_state_dict(base_dict)\n",
    "\n",
    "input_size = net.scale_size if full_res else net.input_size\n",
    "if test_crops == 1:\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupScale(net.scale_size),\n",
    "        GroupCenterCrop(input_size),\n",
    "    ])\n",
    "elif test_crops == 3:  # do not flip, so only 5 crops\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupFullResSample(input_size, net.scale_size, flip=False)\n",
    "    ])\n",
    "elif test_crops == 5:  # do not flip, so only 5 crops\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupOverSample(input_size, net.scale_size, flip=False)\n",
    "    ])\n",
    "elif test_crops == 10:\n",
    "    cropping = torchvision.transforms.Compose([\n",
    "        GroupOverSample(input_size, net.scale_size)\n",
    "    ])\n",
    "else:\n",
    "    raise ValueError(\"Only 1, 5, 10 crops are supported while we got {}\".format(test_crops))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data and create data loaders\n",
    "Just like the training data, our input videos need to be converted to clips and then frames (images) first.\n",
    "We will split the video into clips of equal length (determined by the CLIP_DURATION parameter).\n",
    "These clips will then be converted into frames.\n",
    "Then we will create a dataloader per-video, to be used for prediction later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_video_properties(video_path):\n",
    "    #get video properties\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)      # OpenCV2 version 2 used \"CV_CAP_PROP_FPS\"\n",
    "    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))   # float `width`\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) ) # float `height`\n",
    "    cap.release()\n",
    "    return fps, frame_count, width, height\n",
    "\n",
    "def prepare_video(video_path, preprocess = True, dataloaders = True, del_clips = True):\n",
    "    filename = os.path.basename(video_path)\n",
    "    \n",
    "    #get video properties\n",
    "    fps, frame_count, width, height = get_video_properties(video_path)\n",
    "    \n",
    "    \n",
    "    dest_dir = os.path.join(tmp_dir, filename)\n",
    "    print(dest_dir)\n",
    "    print(f'fps: {fps}, frame count: {frame_count}, clip duration: {CLIP_DURATION}, filename: {filename}')\n",
    "    \n",
    "    if preprocess:\n",
    "        for i in range(0, int(frame_count/fps), int(CLIP_DURATION)):\n",
    "            Path(dest_dir).mkdir(exist_ok=True)\n",
    "            target_filepath = os.path.join(dest_dir, f'{i:04}_{filename}')\n",
    "            \n",
    "            #convert video to clips\n",
    "            ffmpeg_extract_subclip(video_path, i, i + int(CLIP_DURATION), targetname=target_filepath)\n",
    "\n",
    "        #convert clips to images\n",
    "        convert_folder(dest_dir, dest_dir)\n",
    "    \n",
    "    #generate video file list\n",
    "    video_folders_file = 'videofolder.txt'\n",
    "    video_folders_filepath = os.path.join(dest_dir, video_folders_file)\n",
    "    with open(video_folders_filepath, 'w+') as file:\n",
    "        for folder in glob.glob(os.path.join(dest_dir, '*')):\n",
    "            if not os.path.isdir(folder):\n",
    "                continue\n",
    "            num_images = len(glob.glob(os.path.join(folder, '*')))\n",
    "            file.write(f'{folder},{num_images},{-1}\\n')\n",
    "            \n",
    "    if del_clips:\n",
    "        for p in glob.glob(os.path.join(dest_dir, '*.MP4')):\n",
    "            os.remove(p)\n",
    "            \n",
    "    #prepare data loaders\n",
    "    if dataloaders:\n",
    "        print(f'net.input_mean: {net.input_mean}, net.input_std: {net.input_std}')\n",
    "        data_loader = torch.utils.data.DataLoader(\n",
    "                    TSNDataSet(root_path, video_folders_filepath, num_segments=num_segments,\n",
    "                               new_length=1 if modality == \"RGB\" else 5,\n",
    "                               modality=modality,\n",
    "                               image_tmpl=prefix,\n",
    "                               test_mode=True,\n",
    "                               random_shift = False, #use consistent spacing between segments (frames)\n",
    "                               transform=torchvision.transforms.Compose([\n",
    "                                   cropping,\n",
    "                                   Stack(roll=(this_arch in ['BNInception', 'InceptionV3'])),\n",
    "                                   ToTorchFormatTensor(div=(this_arch not in ['BNInception', 'InceptionV3'])),\n",
    "                                   GroupNormalize([0.485, 0.456, 0.406], [0.485, 0.456, 0.406]),\n",
    "                               ]), dense_sample=dense_sample, twice_sample=twice_sample),\n",
    "                    batch_size=batch_size, shuffle=False,\n",
    "                    num_workers=num_workers, pin_memory=True,\n",
    "            )\n",
    "    else:\n",
    "        data_loader = None\n",
    "        \n",
    "    return {'data_loader': data_loader, 'videofolder':video_folders_filepath, 'fps':fps, 'frame_count':frame_count, 'width': width, 'height':height}\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\Desktop\\projects\\ITE_APAMS\\ite_dataset\\tmp\\A_1.MP4\n",
      "fps: 29.97002997002997, frame count: 1332, clip duration: 2, filename: A_1.MP4\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 22/22 [00:11<00:00,  1.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:22\n",
      "C:\\Users\\User1\\Desktop\\projects\\ITE_APAMS\\ite_dataset\\tmp\\A_2.MP4\n",
      "fps: 29.97002997002997, frame count: 2979, clip duration: 2, filename: A_2.MP4\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 50/50 [00:34<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:50\n",
      "C:\\Users\\User1\\Desktop\\projects\\ITE_APAMS\\ite_dataset\\tmp\\A_3.MP4\n",
      "fps: 29.97002997002997, frame count: 1382, clip duration: 2, filename: A_3.MP4\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n",
      "Moviepy - Running:\n",
      ">>> \"+ \" \".join(cmd)\n",
      "Moviepy - Command successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 23/23 [00:17<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "net.input_mean: [0.485, 0.456, 0.406], net.input_std: [0.485, 0.456, 0.406]\n",
      "=> Using dense sample for the dataset...\n",
      "video number:23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "video_dict = dict()\n",
    "for video_path in video_paths:\n",
    "    # if you have previously run this code and have the converted clips and frames in tmp_dir, then you can set preprocess=False\n",
    "    video_dict[video_path] = prepare_video(video_path, preprocess = True, dataloaders = True, del_clips=False) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User1\\anaconda3\\envs\\ptcpu\\lib\\site-packages\\torch\\nn\\functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  ..\\c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    }
   ],
   "source": [
    "net = torch.nn.DataParallel(net.to(dev))\n",
    "net.eval()\n",
    "for video_path in video_paths:\n",
    "    data_loader = video_dict[video_path]['data_loader']\n",
    "    data_gen = enumerate(data_loader)\n",
    "    this_rst_list = []\n",
    "    label_list = []\n",
    "    for i, (data, label) in data_gen:\n",
    "        rst = eval_video((i, data, label), net, this_test_segments, modality)\n",
    "\n",
    "        for l, r in zip(label, rst[1]): #unpack batch to individual samples\n",
    "            #save to lists\n",
    "            this_rst_list.append(r) \n",
    "            label_list.append(l) \n",
    "            \n",
    "    video_dict[video_path]['preds']= this_rst_list\n",
    "    video_dict[video_path]['labels']= label_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save using pickle.....have to stop this to do object placement\n",
    "import pickle\n",
    "Path(output_path).mkdir(exist_ok=True)\n",
    "file_to_write = open( os.path.join(output_path, 'prediction_results'), \"wb\")\n",
    "pickle.dump(video_dict, file_to_write)\n",
    "\n",
    "file_to_write.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading video_dict and all its predictions so as to not go through predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### variables required?\n",
    "cwd = \n",
    "output_path =  os.path.join(cwd, f'{dt}_{dataset_version}_nseg{num_segments}_ITE_VIDEO_results')\n",
    "\n",
    "##### please load categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import pickle\n",
    "\n",
    "file_to_read = open(os.path.join(output_path, 'prediction_results'), \"rb\")\n",
    "\n",
    "video_dict = pickle.load(file_to_read)\n",
    "\n",
    "file_to_read.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load label map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read label map:\n",
    "with open(os.path.join(root_data_path, dataset_version, 'actions_label_map.txt'), 'r') as file:\n",
    "    categories = file.readlines()\n",
    "    categories = [c.strip().replace(' ', '_').replace('\"', '').replace('(', '').replace(')', '').replace(\"'\", '') for c in categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['connect_alligator_clip',\n",
       " 'connect_atx_cable',\n",
       " 'connect_display_cable',\n",
       " 'connect_hdd_data_cable',\n",
       " 'connect_hdd_power_cable',\n",
       " 'connect_odd_data_cable',\n",
       " 'connect_odd_power_cable',\n",
       " 'disconnect_atx_cable',\n",
       " 'disconnect_display_cable',\n",
       " 'disconnect_hdd_data_cable',\n",
       " 'disconnect_hdd_power_cable',\n",
       " 'disconnect_odd_data_cable',\n",
       " 'disconnect_odd_power_cable',\n",
       " 'enter_bios_setup_mode',\n",
       " 'insert_hdd',\n",
       " 'insert_odd',\n",
       " 'insert_ram',\n",
       " 'insert_vga',\n",
       " 'login_screen',\n",
       " 'no_action',\n",
       " 'place_anti_static_mat',\n",
       " 'put_back_pc_casing',\n",
       " 'remove_hdd',\n",
       " 'remove_odd',\n",
       " 'remove_pc_casing',\n",
       " 'remove_ram',\n",
       " 'remove_vga',\n",
       " 'switch_off_power',\n",
       " 'switch_off_power_source',\n",
       " 'turn_on_pc',\n",
       " 'unplug_power_cable',\n",
       " 'verify_boot_sequence',\n",
       " 'wear_wrist_wrap']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_preds_from_csv(csv_path):\n",
    "    preds = list()\n",
    "    with open(csv_path) as file:\n",
    "        for line in file.readlines()[1:]:\n",
    "            pred = line.strip().split(',')[1:]\n",
    "#             pred = [float(i) for i in str_pred.split(',')]\n",
    "            preds.append(pred)\n",
    "    return preds\n",
    "\n",
    "def process_preds(preds, softmax_thresh=False, suppress_to = None):\n",
    "    pred_idxs = []\n",
    "    for pred in preds:\n",
    "        if softmax_thresh:\n",
    "            if max(pred)<softmax_thresh:\n",
    "                pred_idxs.append(suppress_to)\n",
    "            else:\n",
    "                pred_idxs.append(np.argmax(pred))\n",
    "    return pred_idxs\n",
    "\n",
    "def get_clips_paths(video_folders_filepath):\n",
    "    #get video clip paths from videofolder.txt\n",
    "    clip_paths = list()\n",
    "    with open(video_folders_filepath, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            clip_path = line.split(',')[0]+'.MP4'\n",
    "            clip_paths.append(clip_path)\n",
    "    return clip_paths\n",
    "\n",
    "def get_clips_lengths(video_folders_filepath):\n",
    "    #get video clip paths from videofolder.txt\n",
    "    clip_lengths = list()\n",
    "    with open(video_folders_filepath, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            clip_num_frames = int(line.split(',')[1])\n",
    "            clip_lengths.append(clip_num_frames)\n",
    "    return clip_lengths\n",
    "\n",
    "def save_preds_to_csv(video_path):\n",
    "    file_name = os.path.basename(video_path)\n",
    "    video_folders_filepath = video_dict[video_path]['videofolder']\n",
    "#     print(video_folders_filepath)\n",
    "    preds = video_dict[video_path]['preds']\n",
    "    labels = video_dict[video_path]['labels']\n",
    "        \n",
    "    #get video clip paths from videofolder.txt\n",
    "    clip_lengths = get_clips_lengths(video_folders_filepath)\n",
    "    print('number of preds: ', len(preds), 'number of clips: ',len(clip_lengths))\n",
    "    assert len(preds)==len(clip_lengths)\n",
    "    \n",
    "    # write results to csv file\n",
    "    csv_filepath = os.path.join(output_path, 'pred_'+file_name+'.csv')\n",
    "    csv_file = open(csv_filepath, 'w+')\n",
    "    cats_string = ','.join(categories)\n",
    "    csv_file.write(f'frame_pos,{cats_string}\\n')\n",
    "    i = 1\n",
    "    for raw_pred, label, clip_length in zip(preds, labels, clip_lengths):\n",
    "        #used to open clip to find number of frames\n",
    "        # now will just use value from videofolder.txt file to find that number\n",
    "        #get video properties\n",
    "        fps, frame_count, width, height = get_video_properties(video_path)\n",
    "        if clip_length > fps*CLIP_DURATION:\n",
    "            clip_length = fps*CLIP_DURATION #to avoid this problem: https://video.stackexchange.com/questions/23373/ffmpeg-not-creating-exact-duration-clip\n",
    "            \n",
    "        for j in range(int(clip_length)):\n",
    "            pred_str = ','.join([str(i) for i in raw_pred])\n",
    "            csv_file.write(f'{i},{pred_str}\\n')\n",
    "            i+= 1\n",
    "    csv_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of preds:  22 number of clips:  22\n",
      "number of preds:  50 number of clips:  50\n",
      "number of preds:  23 number of clips:  23\n"
     ]
    }
   ],
   "source": [
    "for video_path in video_paths:\n",
    "    save_preds_to_csv(video_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process predictions \n",
    "Split by section using time, then suppress\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONF_THRES = 0.4 # previously 0.5 lead to worse scores than no confidence threshold\n",
    "USE_CONF_THRES = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for video_path in video_paths:\n",
    "    #read predictions from csv:\n",
    "    file_name = os.path.basename(video_path)\n",
    "    csv_filepath = os.path.join(output_path, 'pred_'+ file_name +'.csv')\n",
    "    preds = get_preds_from_csv(csv_filepath)    \n",
    "\n",
    "    #get video properties\n",
    "    fps, frame_count, width, height = get_video_properties(video_path)\n",
    "    \n",
    "    #process predictions\n",
    "    processed_preds = list()\n",
    "    predicted_actions = list()\n",
    "    pred_act_sect = list()\n",
    "    per_frame_sections = list()\n",
    "    processed_preds_unmasked = list()\n",
    "    \n",
    "    for idx, pred in enumerate(preds):\n",
    "        pred = [float(p) for p in pred]\n",
    "        #current minute into the video\n",
    "        current_minute = idx/fps/60\n",
    "        \n",
    "      \n",
    "        if USE_CONF_THRES:\n",
    "            # for actions (suppressed or not), check whether it meets a confidence criteria \n",
    "            if np.max(masked_preds) > CONF_THRES:\n",
    "                predicted_action = categories[np.argmax(pred)]      \n",
    "            else:\n",
    "                predicted_action = 'no_action'\n",
    "        else:\n",
    "            #get predicted action --- ORIGINAL, 22/6/2021\n",
    "            predicted_action = categories[np.argmax(pred)]\n",
    "            \n",
    "            \n",
    "            \n",
    "        predicted_actions.append(predicted_action)\n",
    "        \n",
    "    video_dict[video_path]['predicted_actions'] = predicted_actions\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write processed predictions to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1298\n",
      "2932\n",
      "1357\n"
     ]
    }
   ],
   "source": [
    "for video_path in video_paths:\n",
    "    #read predictions from csv:\n",
    "    file_name = os.path.basename(video_path)\n",
    "    csv_filepath = os.path.join(output_path, 'processed_pred_'+file_name+'.csv')\n",
    "    preds = video_dict[video_path]['predicted_actions']\n",
    "    print(len(preds))\n",
    "    with open(csv_filepath, 'w+') as file:\n",
    "        file.write(f'frame_pos,pred\\n')\n",
    "        for idx, pred in enumerate(preds):\n",
    "            file.write(f'{idx+1},{pred}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write predictions to video\n",
    "### warning: will take long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing A_1.MP4\n",
      "1298 22\n",
      "1298 1332\n",
      "writing to: C:\\Users\\User1\\Desktop\\projects\\ITE_APAMS\\action_recoginition\\202108271426_pc_101_output\\pred_A_1.MP4\n",
      "pred limit reached. i=1298, clip_path = C:\\Users\\User1\\Desktop\\projects\\ITE_APAMS\\ite_dataset\\tmp\\A_1.MP4\\0042_A_1.MP4\n"
     ]
    }
   ],
   "source": [
    "#TODO: read preds from processed_preds csv file \n",
    "# def write_preds_to_video(video_path):\n",
    "for video_path in video_paths[0:1]: #video_paths[14:19] - TO SELECT A FEW\n",
    "    \n",
    "    file_name = os.path.basename(video_path)\n",
    "    print(f'processing {file_name}')\n",
    "    video_folders_filepath = video_dict[video_path]['videofolder']\n",
    "    \n",
    "    #get preds\n",
    "    preds = video_dict[video_path]['predicted_actions']\n",
    "\n",
    "    #get video clip paths from videofolder.txt\n",
    "    clip_paths = get_clips_paths(video_folders_filepath)\n",
    "    print(len(preds), len(clip_paths))\n",
    "    \n",
    "    fps, frame_count, width, height = get_video_properties(video_path)\n",
    "    print(len(preds),frame_count)\n",
    "    label_height = 60\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'MP4V')\n",
    "    \n",
    "    output_video_filepath = os.path.join(output_path, 'pred_'+file_name)\n",
    "    out = cv2.VideoWriter(output_video_filepath ,fourcc, fps, (width, height+label_height))\n",
    "    print(f'writing to: {output_video_filepath}')\n",
    "\n",
    "    i=0\n",
    "    for clip_path in clip_paths:        \n",
    "        \n",
    "        #open clip video\n",
    "        current_clip_frame_count = 0\n",
    "        cap = cv2.VideoCapture(clip_path) \n",
    "        while True:\n",
    "            _, img = cap.read()  \n",
    "\n",
    "            if not _:\n",
    "#                 print(\"No Image\")\n",
    "                cap.release()\n",
    "                break\n",
    "            elif current_clip_frame_count >= CLIP_DURATION*fps:\n",
    "                cap.release()\n",
    "                break\n",
    "            elif i >= len(preds):\n",
    "                print(f'pred limit reached. i={i}, clip_path = {clip_path}')\n",
    "                cap.release()\n",
    "                break\n",
    "            pred = preds[i]\n",
    "\n",
    "            \n",
    "#             print(current_section)\n",
    "#             if i%100==0:\n",
    "#                 print(pred)\n",
    "\n",
    "            label_area = np.zeros([label_height, width, 3]).astype('uint8') + 255\n",
    "    \n",
    "#             print(pred)\n",
    "\n",
    "            cv2.putText(label_area, \n",
    "                        f'Prediction: {pred}.  ',# ({max(pred):.6f}) Label: {labels[i]}',\n",
    "                        (5, int(label_height-20)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                        0.7, \n",
    "                        (0, 0, 0), \n",
    "                        2)\n",
    "            img = np.concatenate((img, label_area), axis=0)\n",
    "            \n",
    "            out.write(img)\n",
    "            i+= 1\n",
    "            current_clip_frame_count += 1\n",
    "    out.release()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score predictions\n",
    "## Only possible for annotated videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_1.MP4\n",
      "1298 1332\n",
      "A_2.MP4\n",
      "2932 2979\n",
      "A_3.MP4\n",
      "1357 1382\n"
     ]
    }
   ],
   "source": [
    "\n",
    "iou_means = list()\n",
    "iou_per_class_all_videos_df = pd.DataFrame(columns=categories)\n",
    "\n",
    "for video_path in video_paths:\n",
    "    file_name = os.path.basename(video_path)\n",
    "    print(file_name)\n",
    "    \n",
    "    # read GT and convert to per-frame labels\n",
    "    gt_csv = video_path +'.csv'\n",
    "    gt_labels = pd.read_csv(gt_csv)\n",
    "    #remove no_action rows\n",
    "    gt_labels = gt_labels[gt_labels['action']!='no_action']\n",
    "    gt_labels.sort_values('z_start', inplace=True)\n",
    "    #get frame count and fps of video\n",
    "    frame_count = video_dict[video_path]['frame_count']\n",
    "    fps = video_dict[video_path]['fps']\n",
    "#     base_data = np.zeros((frame_count, len(categories)))\n",
    "    gt_labels_onehot = pd.DataFrame(0, index=np.arange(frame_count), columns=categories)\n",
    "    gt_labels_onehot.no_action = 1\n",
    "    for index, row in gt_labels.iterrows():\n",
    "        action, z_start, z_end = row\n",
    "        \n",
    "        #ignore action if it isn't in actions_label_map.txt\n",
    "        if action not in categories:\n",
    "            continue\n",
    "#         print(action, z_start, z_end)\n",
    "        frame_pos_start = int(z_start*fps)\n",
    "        frame_pos_end = int(z_end*fps)\n",
    "#         print(frame_pos_start, frame_pos_end)\n",
    "        gt_labels_onehot[action][frame_pos_start:frame_pos_end]=1\n",
    "        gt_labels_onehot.no_action[frame_pos_start:frame_pos_end]=0\n",
    "        \n",
    "    #read predictions file\n",
    "    output_csv_filepath = os.path.join(output_path, 'processed_pred_'+ file_name + '.csv')\n",
    "    preds_per_frame = pd.read_csv(output_csv_filepath)\n",
    "    print(len(preds_per_frame), len(gt_labels_onehot))\n",
    "    \n",
    "    #convert to one-hot labels\n",
    "    preds_onehot = pd.DataFrame(0, index=np.arange(frame_count), columns=categories)\n",
    "    for index, row in preds_per_frame.iterrows():\n",
    "        frame_pos, action = row\n",
    "#         print(frame_pos, action)\n",
    "        preds_onehot[action][frame_pos-1] = 1\n",
    "    \n",
    "    intersection_per_class = (gt_labels_onehot*preds_onehot).sum()\n",
    "    union_per_class = (gt_labels_onehot|preds_onehot).sum()\n",
    "#     union_per_class = gt_labels_onehot.sum()+preds_onehot.sum()\n",
    "    iou_per_class = intersection_per_class/union_per_class\n",
    "    mean_iou = iou_per_class.mean()\n",
    "#     print(f'intersection: {intersection}, union: {union}')\n",
    "    iou_means.append(mean_iou)\n",
    "    iou_per_class_all_videos_df=iou_per_class_all_videos_df.append(iou_per_class, ignore_index=True)\n",
    "#     break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iou_per_class_all_videos_df.to_csv('iou_per_class_all_videos.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou mean per video:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    0.648002\n",
       "1    0.505065\n",
       "2    0.487372\n",
       "dtype: float64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('iou mean per video:')\n",
    "iou_per_class_all_videos_df.mean(axis=1, skipna=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iou mean per class:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "connect_alligator_clip             NaN\n",
       "connect_atx_cable                  NaN\n",
       "connect_display_cable              NaN\n",
       "connect_hdd_data_cable             NaN\n",
       "connect_hdd_power_cable            NaN\n",
       "connect_odd_data_cable             NaN\n",
       "connect_odd_power_cable            NaN\n",
       "disconnect_atx_cable               NaN\n",
       "disconnect_display_cable           NaN\n",
       "disconnect_hdd_data_cable          NaN\n",
       "disconnect_hdd_power_cable         NaN\n",
       "disconnect_odd_data_cable          NaN\n",
       "disconnect_odd_power_cable         NaN\n",
       "enter_bios_setup_mode              NaN\n",
       "insert_hdd                         NaN\n",
       "insert_odd                         NaN\n",
       "insert_ram                         NaN\n",
       "insert_vga                         NaN\n",
       "login_screen                  0.253252\n",
       "no_action                     0.906280\n",
       "place_anti_static_mat              NaN\n",
       "put_back_pc_casing                 NaN\n",
       "remove_hdd                         NaN\n",
       "remove_odd                         NaN\n",
       "remove_pc_casing                   NaN\n",
       "remove_ram                         NaN\n",
       "remove_vga                         NaN\n",
       "switch_off_power                   NaN\n",
       "switch_off_power_source            NaN\n",
       "turn_on_pc                    0.480907\n",
       "unplug_power_cable                 NaN\n",
       "verify_boot_sequence               NaN\n",
       "wear_wrist_wrap                    NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('iou mean per class:')\n",
    "iou_per_class_all_videos_df.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model score:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5468131124399395"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('model score:')\n",
    "iou_per_class_all_videos_df.mean(axis=1, skipna=True).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model output analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "maxes = []\n",
    "for video_path in video_paths:\n",
    "#     print(video_dict[video_path])\n",
    "    file_name = os.path.basename(video_path)\n",
    "    preds = video_dict[video_path]['preds']\n",
    "    for pred in preds:\n",
    "        maxes.append(max(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(maxes, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whiskers': [<matplotlib.lines.Line2D at 0x19980175e50>,\n",
       "  <matplotlib.lines.Line2D at 0x199801cc040>],\n",
       " 'caps': [<matplotlib.lines.Line2D at 0x199801cc310>,\n",
       "  <matplotlib.lines.Line2D at 0x199801cc880>],\n",
       " 'boxes': [<matplotlib.lines.Line2D at 0x19980175a60>],\n",
       " 'medians': [<matplotlib.lines.Line2D at 0x199801ccb20>],\n",
       " 'fliers': [<matplotlib.lines.Line2D at 0x199801ccc10>],\n",
       " 'means': []}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plt.boxplot(maxes)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
